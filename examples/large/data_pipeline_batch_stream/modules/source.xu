// Data Source Module
// Defines various data sources for the pipeline

// File data source
FileDataSource has {
    file_path: string
    delimiter: string
    has_header: bool
    current_line: int
    total_lines: int
    data: [any]

    init(file_path: string, delimiter: string = ",", has_header: bool = true) {
        self.file_path = file_path
        self.delimiter = delimiter
        self.has_header = has_header
        self.current_line = 0
        self.total_lines = 1000 // Simulated total lines
        self.data = []
        self._initialize_data()
    }

    func _initialize_data() {
        // Simulate loading data from file
        for i in 0..999 {
            self.data.add([i, "item_{i}", i * 10.5, i % 2 == 0])
        }
    }

    func read_batch() -> [any] {
        let batch_size = 100
        let end = min(self.current_line + batch_size, self.total_lines)
        let batch = self.data.slice(self.current_line, end)
        self.current_line = end
        return batch
    }

    func read_stream() -> any {
        if self.current_line < self.total_lines {
            let item = self.data[self.current_line]
            self.current_line = self.current_line + 1
            return item
        }
        return null
    }

    func has_more() -> bool {
        return self.current_line < self.total_lines
    }

    func close() {
        println("Closed file data source: {self.file_path}")
    }

    static func create(file_path: string, delimiter: string = ",", has_header: bool = true) -> FileDataSource {
        return FileDataSource{ file_path: file_path, delimiter: delimiter, has_header: has_header }
    }
}

// API data source
ApiDataSource has {
    api_url: string
    api_key: string
    page: int
    page_size: int
    total_items: int
    items_fetched: int

    init(api_url: string, api_key: string = "", page_size: int = 50) {
        self.api_url = api_url
        self.api_key = api_key
        self.page = 1
        self.page_size = page_size
        self.total_items = 800 // Simulated total items
        self.items_fetched = 0
    }

    func read_batch() -> [any] {
        let batch = []
        let remaining = min(self.page_size, self.total_items - self.items_fetched)
        
        for i in 0..remaining-1 {
            batch.add({
                id: self.items_fetched + i,
                name: "API Item {self.items_fetched + i}",
                value: (self.items_fetched + i) * 2.5,
                timestamp: System.currentTimeMillis()
            })
        }
        
        self.items_fetched = self.items_fetched + remaining
        self.page = self.page + 1
        return batch
    }

    func read_stream() -> any {
        if self.items_fetched < self.total_items {
            let item = {
                id: self.items_fetched,
                name: "API Item {self.items_fetched}",
                value: self.items_fetched * 2.5,
                timestamp: System.currentTimeMillis()
            }
            self.items_fetched = self.items_fetched + 1
            return item
        }
        return null
    }

    func has_more() -> bool {
        return self.items_fetched < self.total_items
    }

    func close() {
        println("Closed API data source: {self.api_url}")
    }

    static func create(api_url: string, api_key: string = "", page_size: int = 50) -> ApiDataSource {
        return ApiDataSource{ api_url: api_url, api_key: api_key, page_size: page_size }
    }
}

// Database data source
DatabaseDataSource has {
    connection_string: string
    query: string
    params: [any]
    current_row: int
    total_rows: int
    result_set: [any]

    init(connection_string: string, query: string, params: [any] = []) {
        self.connection_string = connection_string
        self.query = query
        self.params = params
        self.current_row = 0
        self.total_rows = 1200 // Simulated total rows
        self.result_set = []
        self._execute_query()
    }

    func _execute_query() {
        // Simulate executing query and fetching results
        for i in 0..1199 {
            self.result_set.add({
                id: i,
                user_id: i % 100,
                product_id: (i % 500) + 1,
                quantity: (i % 10) + 1,
                price: ((i % 50) + 1) * 10.0,
                order_date: "2024-01-{i % 30 + 1}"
            })
        }
    }

    func read_batch() -> [any] {
        let batch_size = 150
        let end = min(self.current_row + batch_size, self.total_rows)
        let batch = self.result_set.slice(self.current_row, end)
        self.current_row = end
        return batch
    }

    func read_stream() -> any {
        if self.current_row < self.total_rows {
            let item = self.result_set[self.current_row]
            self.current_row = self.current_row + 1
            return item
        }
        return null
    }

    func has_more() -> bool {
        return self.current_row < self.total_rows
    }

    func close() {
        println("Closed database data source")
    }

    static func create(connection_string: string, query: string, params: [any] = []) -> DatabaseDataSource {
        return DatabaseDataSource{ connection_string: connection_string, query: query, params: params }
    }
}

// Kafka data source
KafkaDataSource has {
    bootstrap_servers: string
    topic: string
    group_id: string
    current_offset: int
    total_messages: int

    init(bootstrap_servers: string, topic: string, group_id: string = "pipeline-group") {
        self.bootstrap_servers = bootstrap_servers
        self.topic = topic
        self.group_id = group_id
        self.current_offset = 0
        self.total_messages = 2000 // Simulated total messages
    }

    func read_batch() -> [any] {
        let batch_size = 200
        let end = min(self.current_offset + batch_size, self.total_messages)
        let batch = []
        
        for i in self.current_offset..end-1 {
            batch.add({
                offset: i,
                key: "key_{i % 100}",
                value: "message_{i}",
                timestamp: System.currentTimeMillis() - (self.total_messages - i) * 100
            })
        }
        
        self.current_offset = end
        return batch
    }

    func read_stream() -> any {
        if self.current_offset < self.total_messages {
            let message = {
                offset: self.current_offset,
                key: "key_{self.current_offset % 100}",
                value: "message_{self.current_offset}",
                timestamp: System.currentTimeMillis() - (self.total_messages - self.current_offset) * 100
            }
            self.current_offset = self.current_offset + 1
            return message
        }
        return null
    }

    func has_more() -> bool {
        return self.current_offset < self.total_messages
    }

    func close() {
        println("Closed Kafka data source: {self.topic}")
    }

    static func create(bootstrap_servers: string, topic: string, group_id: string = "pipeline-group") -> KafkaDataSource {
        return KafkaDataSource{ bootstrap_servers: bootstrap_servers, topic: topic, group_id: group_id }
    }
}

// In-memory data source
InMemoryDataSource has {
    data: [any]
    current_index: int

    init(data: [any]) {
        self.data = data
        self.current_index = 0
    }

    func read_batch() -> [any] {
        let batch_size = 50
        let end = min(self.current_index + batch_size, self.data.length)
        let batch = self.data.slice(self.current_index, end)
        self.current_index = end
        return batch
    }

    func read_stream() -> any {
        if self.current_index < self.data.length {
            let item = self.data[self.current_index]
            self.current_index = self.current_index + 1
            return item
        }
        return null
    }

    func has_more() -> bool {
        return self.current_index < self.data.length
    }

    func close() {
        println("Closed in-memory data source")
    }

    static func create(data: [any]) -> InMemoryDataSource {
        return InMemoryDataSource{ data: data }
    }
}
